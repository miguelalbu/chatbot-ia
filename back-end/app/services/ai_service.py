from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.config.settings import settings
from app.services.sql_service import SQLService
from app.services.rag_service import RAGService
from app.services.memory_service import MemoryService

class AIService:
    def __init__(self):
        if not settings.GOOGLE_API_KEY:
            raise ValueError("API Key do Google n√£o configurada!")
        
        # 1. Configura√ß√£o do LLM via LangChain
        # Usamos temperature=0.7 para ele ser criativo mas n√£o alucinar dados
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0.7 
        )
        
        # 2. Inje√ß√£o de Depend√™ncias (Seus servi√ßos robustos)
        self.sql_service = SQLService()
        self.rag_service = RAGService()
        self.memory_service = MemoryService()

        # 3. Defini√ß√£o do Prompt Template (Padr√£o LangChain)
        self.prompt_template = PromptTemplate.from_template("""
        Voc√™ √© um consultor especialista e vendedor da Loja de Perfumes 'Royal Scents'.
        
        >>> CONTEXTO DE DADOS (Fatos Reais) <<<
        ESTOQUE E PRE√áOS (SQL):
        {sql_context}
        
        MANUAL E POL√çTICAS (RAG):
        {rag_context}
        
        >>> HIST√ìRICO DA CONVERSA <<<
        {history}
        
        >>> NOVA MENSAGEM DO CLIENTE <<<
        {query}
        
        >>> DIRETRIZES DE RESPOSTA <<<
        1. Responda de forma fluida, amig√°vel e persuasiva.
        2. Use ESTRITAMENTE os dados do SQL para pre√ßos e estoque.
        3. Use os dados do RAG para descri√ß√µes, dicas e regras.
        4. Se n√£o souber, diga que vai verificar com o gerente, n√£o invente.
        5. Tente fechar a venda sugerindo produtos dispon√≠veis.
        """)

        # 4. Cria√ß√£o da Chain (Cadeia de Processamento)
        # Prompt -> LLM -> Texto Puro
        self.chain = self.prompt_template | self.llm | StrOutputParser()

    def generate_response(self, user_query: str, session_id: str = "usuario_padrao") -> str:
        print(f"\n--- [LANGCHAIN] Processando Sess√£o '{session_id}': {user_query} ---")

        # A. Recupera√ß√£o de Contexto (Seus servi√ßos manuais)
        try:
            sql_context = self.sql_service.get_catalog_context()
        except Exception:
            sql_context = "Erro no cat√°logo."

        try:
            rag_context = self.rag_service.search(user_query)
        except Exception:
            rag_context = "Sem dados de base de conhecimento."

        # B. Recupera√ß√£o de Mem√≥ria
        history_tuples = self.memory_service.get_history(session_id)
        history_text = ""
        if history_tuples:
            for role, text in history_tuples[-6:]:
                role_name = "Cliente" if role == "user" else "Vendedor"
                history_text += f"{role_name}: {text}\n"
        else:
            history_text = "In√≠cio da intera√ß√£o."

        # C. Execu√ß√£o da Chain (A m√°gica do LangChain)
        print("üîó [LANGCHAIN] Invocando a Chain...")
        try:
            bot_response = self.chain.invoke({
                "sql_context": sql_context,
                "rag_context": rag_context,
                "history": history_text,
                "query": user_query
            })
            
            # D. Salva na Mem√≥ria
            self.memory_service.add_message(session_id, "user", user_query)
            self.memory_service.add_message(session_id, "model", bot_response)
            
            print("üöÄ [LANGCHAIN] Resposta gerada com sucesso!\n")
            return bot_response

        except Exception as e:
            print(f"‚ùå [ERRO] Falha na Chain: {e}")
            return "Desculpe, tivemos um erro de comunica√ß√£o com nosso sistema central."